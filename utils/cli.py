import os
import logging
from neo4j import GraphDatabase
from typing import Dict, List, Any

# --- 1. é…ç½®å’Œåˆå§‹åŒ– ---

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Neo4jé…ç½® (ä½¿ç”¨æ‚¨åœ¨ kg_builder.py ä¸­æä¾›çš„é…ç½®)
NEO4J_CONFIG = {
    "uri": "neo4j://localhost:7687",
    "auth": ("neo4j", "password"),
    "database": "neo4j",
}


# --- 2. æ¨¡æ‹Ÿ LLM å®¢æˆ·ç«¯å’Œ RAG æ ¸å¿ƒå‡½æ•° ---
class LLMClient:
    """
    æ¨¡æ‹Ÿçš„å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯ï¼Œç”¨äºç”Ÿæˆé—®ç­”å“åº”ã€‚
    ç°åœ¨åŒ…å«æ›´æ™ºèƒ½çš„æ¨¡æ‹Ÿ RAG é€»è¾‘ã€‚
    """

    def __init__(self, model_name="Academic-LLM-RAG-3.1"):
        self.model_name = model_name

    def generate_response(self, question: str, context: str) -> str:
        """
        æ ¹æ®é—®é¢˜å’Œæ£€ç´¢åˆ°çš„çŸ¥è¯†ç”Ÿæˆç­”æ¡ˆ (RAG æ ¸å¿ƒé€»è¾‘)
        """
        # æ¨¡æ‹Ÿ RAG é€»è¾‘ï¼šå¦‚æœæ£€ç´¢åˆ°çŸ¥è¯†ï¼Œåˆ™ä½¿ç”¨çŸ¥è¯†ç”Ÿæˆç­”æ¡ˆ
        if context:
            # ç®€åŒ–å›ç­”æ¨¡æ¿ï¼Œçªå‡º RAG æ•ˆæœ
            return (
                f"\nâœ¨ ã€æ£€ç´¢ç»“æœã€‘\n"
                f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼ˆå…³é”®ä¸‰å…ƒç»„ï¼š{context[:150]}...ï¼‰ï¼Œæˆ‘ä¸ºæ‚¨ç”Ÿæˆäº†ä»¥ä¸‹ä¸“ä¸šå›ç­”ï¼š\n"
                f"å›ç­”ï¼š{self._mock_answer_generation(question, context)}\n"
            )
        else:
            # æ¨¡æ‹Ÿ LLM åœ¨ç¼ºä¹çŸ¥è¯†æ—¶çš„é€šç”¨å›ç­”
            return (
                f"\nâš ï¸ ã€çŸ¥è¯†å›¾è°±æœªå‘½ä¸­ã€‘\n"
                f"æŠ±æ­‰ï¼ŒçŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ä¸ '{question[:40]}...' ç›´æ¥ç›¸å…³çš„å®ä½“æˆ–è®ºæ–‡ã€‚è¿™æ˜¯åŸºäºæˆ‘é¢„è®­ç»ƒçŸ¥è¯†çš„é€šç”¨å›ç­”ï¼š\n"
                f"å›ç­”ï¼š{self._mock_fallback_answer(question)}\n"
            )

    def _mock_answer_generation(self, question: str, context: str) -> str:
        # é»˜è®¤å›é€€é€»è¾‘ï¼ˆå¦‚æœå‘½ä¸­çŸ¥è¯†å›¾è°±ï¼Œä½†æœªå‘½ä¸­å®šåˆ¶é€»è¾‘ï¼‰
        return (
            "é’ˆå¯¹æ‚¨çš„é—®é¢˜ï¼ŒçŸ¥è¯†å›¾è°±å·²æ£€ç´¢åˆ°ç›¸å…³å®ä½“å…³ç³»ï¼Œè¯æ˜ä¿¡æ¯å­˜åœ¨ã€‚è¯·ç›¸ä¿¡è¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å‡†ç¡®çš„å›ç­”ã€‚"
        )

    def _mock_fallback_answer(self, question):
        if "æœ€å…ˆè¿›" in question or "æœ€å¥½" in question:
            return "åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œæ²¡æœ‰ç»å¯¹â€œæœ€å¥½â€çš„æ¨¡å‹ï¼Œä½† Transformer åŠå…¶å˜ä½“ï¼ˆå¦‚ BERT/LLaMAï¼‰æ˜¯å½“å‰ç ”ç©¶çš„ä¸»æµã€‚"
        return f"è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„ NLP é—®é¢˜ï¼Œæ¶‰åŠæ·±åº¦å­¦ä¹ å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒæŠ€æœ¯ã€‚"


# --- 3. çŸ¥è¯†å›¾è°±æ£€ç´¢é€»è¾‘ ---
class KnowledgeGraphQuery:
    """æœ€ç»ˆä¿®æ­£ç‰ˆçš„çŸ¥è¯†å›¾è°±æŸ¥è¯¢é€»è¾‘ï¼Œä½¿ç”¨ labels(node) è·å–å®ä½“ç±»å‹"""

    # æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼Œæ¶µç›– triple.txt ä¸­çš„å…³é”®å®ä½“ï¼Œç”¨äºåˆæ­¥åŒ¹é…
    KNOWN_ENTITIES = [
        "Retrieval Augmented Generation (RAG)", "RAG",
        "Supervised Fine-Tuning (SFT)", "SFT",
        "Direct Preference Optimization (DPO)", "DPO",
        "CXR-RePaiR-Gen", "CXR-RePaiR", "CXR-ReDonE",
        "gpt-4", "gpt-3.5-turbo", "text-davinci-003",
        "radiology report generation", "report writing",
        "MIMIC-CXR", "CXR-PRO", "LLM", "MODEL", "METHOD", "TASK"
    ]

    def __init__(self, uri, auth, database):
        try:
            self.driver = GraphDatabase.driver(uri, auth=auth)
            self.driver.verify_connectivity()
            self.database = database
            # logger.info("Neo4j è¿æ¥æˆåŠŸã€‚") # ä¿æŒè¾“å‡ºæ•´æ´ï¼Œä¸å†é‡å¤
        except Exception as e:
            logger.error(f"æ— æ³•è¿æ¥åˆ° Neo4j æ•°æ®åº“: {e}")
            self.driver = None

    def find_related_knowledge(self, question: str) -> str:
        """
        æ ¸å¿ƒæ£€ç´¢å¢å¼ºé€»è¾‘ï¼šè¯†åˆ«é—®é¢˜ä¸­çš„å…³é”®å®ä½“ï¼Œå¹¶æŸ¥è¯¢å›¾è°±ä¸­ç›¸å…³çš„å…³ç³»ä¸‰å…ƒç»„ã€‚
        """
        if not self.driver:
            return ""

        # 1. å®ä½“è¯†åˆ«å’ŒåŒ¹é…
        found_entities = set()
        for entity in self.KNOWN_ENTITIES:
            if entity.lower() in question.lower():
                found_entities.add(entity)

        if not found_entities:
            return ""

        entity_names_list = list(found_entities)

        # 2. æ„é€  Cypher æŸ¥è¯¢ï¼šä½¿ç”¨ labels(node) è·å–æ ‡ç­¾
        cypher_query = f"""
        UNWIND $entity_names AS name
        MATCH (e1)-[r]->(e2)
        WHERE e1.name = name OR e2.name = name
        RETURN e1.name AS subject, type(r) AS predicate, e2.name AS object, 
               labels(e1)[0] AS subject_type, labels(e2)[0] AS object_type
        LIMIT 10  // é™åˆ¶è¿”å›çš„å…³ç³»æ•°é‡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
        """

        knowledge_snippets = []
        with self.driver.session(database=self.database) as session:
            try:
                # ä¼ é€’å‚æ•°åˆ—è¡¨
                result = session.run(cypher_query, entity_names=entity_names_list)

                # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„å…³ç³»ä¸‰å…ƒç»„
                for record in result:
                    # ç¡®ä¿ subject_type å’Œ object_type ä¸ä¸ºç©º (å³èŠ‚ç‚¹æœ‰æ ‡ç­¾)
                    subject_type = record['subject_type'] if record['subject_type'] else 'None'
                    object_type = record['object_type'] if record['object_type'] else 'None'

                    snippet = (
                        f"[{subject_type}: {record['subject']}] "
                        f"--[{record['predicate']}]--> "
                        f"[{object_type}: {record['object']}]"
                    )
                    knowledge_snippets.append(snippet)

            except Exception as e:
                # æ³¨æ„ï¼šå¦‚æœè¿æ¥æˆ–Cypheræœ¬èº«è¯­æ³•æœ‰é—®é¢˜ï¼Œå¯èƒ½ä¼šåœ¨è¿™é‡Œæ•è·
                logger.error(f"Cypher æŸ¥è¯¢å¤±è´¥: {e}")
                return ""

        if knowledge_snippets:
            # å°†æ£€ç´¢åˆ°çš„æ‰€æœ‰ä¸‰å…ƒç»„ç‰‡æ®µåˆå¹¶æˆä¸€ä¸ªä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
            return " | ".join(knowledge_snippets)
        else:
            return ""

# --- 4. å‘½ä»¤è¡Œæ¥å£ (CLI) ---

class KGRAG_CLI:
    """
    åŸºäºçŸ¥è¯†å›¾è°± RAG çš„äº¤äº’å¼å‘½ä»¤è¡Œæ¥å£
    """

    def __init__(self):
        self.llm = LLMClient()
        self.kg_query = KnowledgeGraphQuery(**NEO4J_CONFIG)
        self.model_name = self.llm.model_name
        self.kg_status = "ã€å·²è¿æ¥ã€‘" if self.kg_query.driver else "ã€æœªè¿æ¥ã€‘"

    def run(self):
        """
        å¯åŠ¨å‘½ä»¤è¡Œå¾ªç¯
        """
        # 1. å¯åŠ¨é—®å€™è¯­ (è‡ªæˆ‘ä»‹ç»)
        print("=" * 60)
        print(f"ğŸ¤– æ¬¢è¿ä½¿ç”¨å­¦æœ¯çŸ¥è¯†é—®ç­”ç³»ç»Ÿå‘½ä»¤è¡Œæ¥å£ (CLI)")
        print("-" * 60)
        print(f"ğŸ’¬ æˆ‘æ˜¯ä¸€ä¸ªå­¦æœ¯é—®ç­”å¤§æ¨¡å‹")
        print(
            f"ğŸ“– æˆ‘å¯ä»¥è§£å†³çš„é—®é¢˜ï¼šä»å­¦æœ¯è®ºæ–‡çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢äº‹å®ï¼Œå¹¶ç”Ÿæˆå…³äº NLPã€LLM æ¶æ„ã€æ–¹æ³•è®ºå’Œè¯„ä¼°æŒ‡æ ‡ç­‰æ–¹é¢çš„ä¸“ä¸šå›ç­”ã€‚")
        print(f"ğŸ”— çŸ¥è¯†å›¾è°±çŠ¶æ€: {self.kg_status}")
        print("=" * 60)
        print("\nè¯·é—®æ‚¨æœ‰ä»€ä¹ˆå…³äºå­¦æœ¯çŸ¥è¯†çš„é—®é¢˜ï¼Ÿ (è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º)\n")

        # 2. äº¤äº’å¾ªç¯
        while True:
            try:
                question = input(f"ğŸ‘¤ æ‚¨çš„é—®é¢˜ > ")

                if question.lower() in ['exit', 'quit']:
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                    break

                if not question.strip():
                    continue

                # 3. æ‰§è¡Œ RAG æµç¨‹

                # æ£€ç´¢çŸ¥è¯†
                context = self.kg_query.find_related_knowledge(question)

                # ç”Ÿæˆå›ç­”
                response = self.llm.generate_response(question, context)

                print(response)

            except EOFError:
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            except Exception as e:
                logger.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                print("\n[ç³»ç»Ÿé”™è¯¯] æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯ã€‚")


# --- 5. ä¸»ç¨‹åºå…¥å£ ---

if __name__ == "__main__":
    cli = KGRAG_CLI()
    cli.run()